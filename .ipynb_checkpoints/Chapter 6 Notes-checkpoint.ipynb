{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6- Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **memory system** is a hierarchy of storage devices with different capacities, costs and access times. Small, fast **cache memories** nearby the CPU act as staging areas for data and instruction stored in main memory. Main memory stages stored in large disks are staging areas for data stored in networks.  \n",
    "**If you can understand how a system moves data up and down the memory hierarchy, then you can write your application programs so that their data items are stored higher in the hierarchy so that the CPU can access them faster**.  \n",
    "This idea centers around an idea known as **locality**. Programs with good locality tend to access the same set of data over and over or they access nearby items. They also tend to access data from upper levels of memory, and thus run faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Storage Technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Random Access Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Access Memory (RAM)** comes in two varieties, static and dynamic.  \n",
    "**Static RAM (SRAM)** is faster and more expensive. It is used for cache memories, both on and off the CPU chip.  \n",
    "**Dynamic RAM (DRAM)** is slower and cheaper. It is used for the main memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SRAM stores each bit in a **bistable** memory table. Each cell is implemented with a six transistor circuit, which can have one of either two states.   Theoretically, there can be a **metastable state** where the pendulum is directly up, but the smallest disturbance will cause a fall.    \n",
    "Due to the bistable nature of SRAM, the memory cell will retain its value indefinitely even with disturbances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRAM stores each bit as charge on a capacitor. DRAM storage can be very dense, each cell consists of a capacitor and a transistor. The DRAM cell is very sensitive to any distrubance. Once disturbed, it will never recover. Various sources of leakage can cause DRAM to lose its charge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells in a DRAM chip are partitioned into d supercells, each consisting of w DRAM cells. A dxw DRAM stores a total of dw bits of information.  \n",
    "The supercells are organized as a rectangular array with r rows and c columns. rc = d. Each supercell has an address of the form (i,j) where i is the row and j is the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each DRAM chip is connected to circuitry known as the **memory controller** which can transfer w bits to and from each chip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRAM chips are packaged in **memory modules** that plug into expansion slots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRAM and SRAMS are **volatile** in the sense that they lose their information if their supply voltage is turned off. **Nonvolatile memories** retain their information even when they are powered off. They are mostly referred to as ROMs (Read only memories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data flows back and forth between the processor and DRAM memory over shared electrical condiuts called buses. Each transfer of data between CPU and memory is accomplished with a series of steps called a **bus transaction**.  \n",
    "A read transaction transfers data from the main memory to the CPU. A write transaction transfers data from the CPU to the main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Disk Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disks** are storage devices that hold enormous amounts of data.   \n",
    "Disks are constructed from **platters**, which each consist of two sides, or **surfaces**. A rotating **spindle** in the center of the paltter spins at a fixed rotational rate. Each surface has a collectino of cocentric rings called **tracks**. Each tracks are partitioned into a collection of **sectors**. Each sector contains an equal number of data bits encoded into the magnetic material in the sector. Sectors are separated by **gaps** where no data bits are stored. Gaps store formatting bits that identify the sectors.  \n",
    "A disk consists of one or more platters stacked on each other. These will sometimes be referred to as rotating disks to distinguish them from SSDs which have no moving parts.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of bits that can be recorded by the disk is known as the **maximum capacity**. This is determined by the following:  \n",
    "**Recording density**: The number of bits that can be squeezed into a one inch segment of a track.   \n",
    "**Track density**: The number of tracks that can be squeezed into a one inch segment of the radius.   \n",
    "**Areal density**: The product of the recording density and track density.   \n",
    "As areal density increases, the gaps between sectors become pretty large. The modern solution is to use a technique called **multiple zone recording**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The capacity of the disk is given by the following formula:   \n",
    "(bytes/sector) x (number of sectors/track) x (tracks/surface) x (surfaces/platter) x (platters/disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disks read and write data in sector size blocks. The access time for a sector has three main components: seek time, rotational latency and transfer time.   \n",
    "**Seek time**: To read the contents of a target time, the arm first positions the head over the track that contains the sector. The time required to position the arm is known as the seek time.   \n",
    "**Rotational Latency**: Once the head is in position, the drive waits for the first bit of the target sector to pass under the head. The average rotational latency is half of T(max rotation). T(max rotation) can be calculated as follows:   \n",
    "T(max rotation) + (1/RPM) x (60 seconds/min)   \n",
    "**Transfer time**: When the first bit of the target sector is under the head, the drive can begin to read or write contents of the sector. The transfer time on the sector depends on the ratational speed and number of sectors in the track. It can be found with this equation:  \n",
    "T(avg transfer) = (1/RPM) x (1/avg sectors/track) x (60seconds/min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.1.3 Solid State Disks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solid state disks is a storage technology based on flash memory. In SSDs, reading is faster than writing, which is caused by a property of flash memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well written programs have good **locality** which means that they tend to reference data items that are near each other or have been recently referenced.  \n",
    "There are two types of locality:  \n",
    "**Temporal locality**: This is where a memory that is referenced once is likely to be referenced multiple times inthe near future.   \n",
    "**Spatial Locality**: This is where a memory location is referenced once, then a program is likely to reference a nearby memory location in the near future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Locality of References to Program Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Stride-1 Reference Pattern** is a sequential reference pattern (i.e. in an array every element is visited sequentially).   \n",
    "Visiting every kth element is called a **stride-k reference pattern**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Locality of Instruction Fetches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the program instructions are stored in memory and must be fetched, we can also evaluate the locality of a program with respect to the instruction fetches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Summary of Locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programs that repeatedly reference the same variable enjoy good temporal locality.   \n",
    "For programs with stride-k reference patterns, the smaller the stride, the better the spatial locality. Programs that hop around memory with large strides have poor spatial locality.  \n",
    "Loops have good temporal and spatial locality. The smaller the loop body and the greater the number of loop iterations, the better the locality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 The Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Memory Hierarchy can be found in the book)   \n",
    "Storage gets slower, cheaper, and larger as you go down the memory hierarchy. The memory hierarchy is the approach for organizing memory systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6.3.1 Caching in the Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **cache** is a small, fast storage device that acts as a staging area for the data objects stored in larger, slower devices. Using a cache is known as **caching**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole idea of the memory hierarchy is that a device at level k serves as a cache for a larger and slower storage device for level k+1.   \n",
    "Data is always copied back and forth between the levels in block size **transfer units**.  \n",
    "**The block size is fixed between adjacent levels in the hierarchy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a program needs a specific data object from a lower leve, it first looks for data stored in the current level. If it is found, we have a **cache hit**. The program then reads the data directly from the current level, which is faster than reading it from the lower level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not cached at the current level, we have a **cache miss**. When there is a miss, the cache at the current level fetches the data from the lower level, maybe overwriting an existing block if the current level's cache is fll.   \n",
    "The process of overwriting a block is knowen as **replacing** the block. The block that is replaced is referred to as the **victim block**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of different kinds of caches.  \n",
    "The first is called a **cold miss**. This happens when misses occur on an empty cache, which is sometimes referred to as a **cold cache**. Cold misses are important since they are often transient events that might not occur in a **steady state** which is when the cache has been **warmed up** by repeated memory accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is a miss, the cache at the current level must implement a **placement policy** in order to place the block it has retrieved. The most flexible policy is to allow it to be stored in any block.   \n",
    "Restrictive placement policies lead to a **conflict miss** in which the cache is large enough to hold the data objects, but since they map to the same cache block, the cache keeps missing. \n",
    "Programs often run as a sequence of phases where each phase accesses some reasonably constant set of cache blocks, which is known as the **working set**. For example a nested loop might access the same elements over and over again.    \n",
    "Then the size of the working set exceeds the size of the cache, the cache will experience what is known as a **capacity miss**. In other words, the cache is just too small to handle the working set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Cache Memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the increasing gap between CPU and main memory, systems designers had to make a small SRAM **cache memory**, an L1 cache between the CPU register file and the main memory.   \n",
    "As the performance gap between the CPU and main memory continued to increase, systems designers responded by inserting a larger cache called an **L2 cache** between the L1 cache and main memory that could be accessed in **10 clock cycles**.   \n",
    "Many modern systems include an even larger cache called an **L3 cache** which sits between the L2 cache and main memory and can be accessed in about **50 cycles**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 Generic Cache Memory Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Writing Cache-Friendly code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmers try to write code that is cache friendly, so the basic approach is to:   \n",
    "**Make the common case go fast**: Programs spend most of the time on a few core functions. Focus on the inner loops of the core functions.   \n",
    "**Minimize the number of cache misses in the loop**: Loops with better miss rates will run faster assuming all other things being equal (i.e. # of loop stores and loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Higher miss rates can have a significant impact on the running time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
