{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6- Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **memory system** is a hierarchy of storage devices with different capacities, costs and access times. Small, fast **cache memories** nearby the CPU act as staging areas for data and instruction stored in main memory. Main memory stages stored in large disks are staging areas for data stored in networks.  \n",
    "**If you can understand how a system moves data up and down the memory hierarchy, then you can write your application programs so that their data items are stored higher in the hierarchy so that the CPU can access them faster**.  \n",
    "This idea centers around an idea known as **locality**. Programs with good locality tend to access the same set of data over and over or they access nearby items. They also tend to access data from upper levels of memory, and thus run faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Storage Technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Random Access Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Access Memory (RAM)** comes in two varieties, static and dynamic.  \n",
    "**Static RAM (SRAM)** is faster and more expensive. It is used for cache memories, both on and off the CPU chip.  \n",
    "**Dynamic RAM (DRAM)** is slower and cheaper. It is used for the main memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SRAM stores each bit in a **bistable** memory table. Each cell is implemented with a six transistor circuit, which can have one of either two states.   Theoretically, there can be a **metastable state** where the pendulum is directly up, but the smallest disturbance will cause a fall.    \n",
    "Due to the bistable nature of SRAM, the memory cell will retain its value indefinitely even with disturbances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRAM stores each bit as charge on a capacitor. DRAM storage can be very dense, each cell consists of a capacitor and a transistor. The DRAM cell is very sensitive to any distrubance. Once disturbed, it will never recover. Various sources of leakage can cause DRAM to lose its charge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells in a DRAM chip are partitioned into d supercells, each consisting of w DRAM cells. A dxw DRAM stores a total of dw bits of information.  \n",
    "The supercells are organized as a rectangular array with r rows and c columns. rc = d. Each supercell has an address of the form (i,j) where i is the row and j is the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each DRAM chip is connected to circuitry known as the **memory controller** which can transfer w bits to and from each chip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRAM chips are packaged in **memory modules** that plug into expansion slots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRAM and SRAMS are **volatile** in the sense that they lose their information if their supply voltage is turned off. **Nonvolatile memories** retain their information even when they are powered off. They are mostly referred to as ROMs (Read only memories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data flows back and forth between the processor and DRAM memory over shared electrical condiuts called buses. Each transfer of data between CPU and memory is accomplished with a series of steps called a **bus transaction**.  \n",
    "A read transaction transfers data from the main memory to the CPU. A write transaction transfers data from the CPU to the main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Disk Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disks** are storage devices that hold enormous amounts of data.   \n",
    "Disks are constructed from **platters**, which each consist of two sides, or **surfaces**. A rotating **spindle** in the center of the paltter spins at a fixed rotational rate. Each surface has a collectino of cocentric rings called **tracks**. Each tracks are partitioned into a collection of **sectors**. Each sector contains an equal number of data bits encoded into the magnetic material in the sector. Sectors are separated by **gaps** where no data bits are stored. Gaps store formatting bits that identify the sectors.  \n",
    "A disk consists of one or more platters stacked on each other. These will sometimes be referred to as rotating disks to distinguish them from SSDs which have no moving parts.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of bits that can be recorded by the disk is known as the **maximum capacity**. This is determined by the following:  \n",
    "**Recording density**: The number of bits that can be squeezed into a one inch segment of a track.   \n",
    "**Track density**: The number of tracks that can be squeezed into a one inch segment of the radius.   \n",
    "**Areal density**: The product of the recording density and track density.   \n",
    "As areal density increases, the gaps between sectors become pretty large. The modern solution is to use a technique called **multiple zone recording**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The capacity of the disk is given by the following formula:   \n",
    "(bytes/sector) x (number of sectors/track) x (tracks/surface) x (surfaces/platter) x (platters/disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disks read and write data in sector size blocks. The access time for a sector has three main components: seek time, rotational latency and transfer time.   \n",
    "**Seek time**: To read the contents of a target time, the arm first positions the head over the track that contains the sector. The time required to position the arm is known as the seek time.   \n",
    "**Rotational Latency**: Once the head is in position, the drive waits for the first bit of the target sector to pass under the head. The average rotational latency is half of T(max rotation). T(max rotation) can be calculated as follows:   \n",
    "T(max rotation) + (1/RPM) x (60 seconds/min)   \n",
    "**Transfer time**: When the first bit of the target sector is under the head, the drive can begin to read or write contents of the sector. The transfer time on the sector depends on the ratational speed and number of sectors in the track. It can be found with this equation:  \n",
    "T(avg transfer) = (1/RPM) x (1/avg sectors/track) x (60seconds/min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.1.3 Solid State Disks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solid state disks is a storage technology based on flash memory. In SSDs, reading is faster than writing, which is caused by a property of flash memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well written programs have good **locality** which means that they tend to reference data items that are near each other or have been recently referenced.  \n",
    "There are two types of locality:  \n",
    "**Temporal locality**: This is where a memory that is referenced once is likely to be referenced multiple times inthe near future.   \n",
    "**Spatial Locality**: This is where a memory location is referenced once, then a program is likely to reference a nearby memory location in the near future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Locality of References to Program Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Stride-1 Reference Pattern** is a sequential reference pattern (i.e. in an array every element is visited sequentially).   \n",
    "Visiting every kth element is called a **stride-k reference pattern**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Locality of Instruction Fetches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the program instructions are stored in memory and must be fetched, we can also evaluate the locality of a program with respect to the instruction fetches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Summary of Locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programs that repeatedly reference the same variable enjoy good temporal locality.   \n",
    "For programs with stride-k reference patterns, the smaller the stride, the better the spatial locality. Programs that hop around memory with large strides have poor spatial locality.  \n",
    "Loops have good temporal and spatial locality. The smaller the loop body and the greater the number of loop iterations, the better the locality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 The Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Memory Hierarchy can be found in the book)   \n",
    "Storage gets slower, cheaper, and larger as you go down the memory hierarchy. The memory hierarchy is the approach for organizing memory systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6.3.1 Caching in the Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **cache** is a small, fast storage device that acts as a staging area for the data objects stored in larger, slower devices. Using a cache is known as **caching**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole idea of the memory hierarchy is that a device at level k serves as a cache for a larger and slower storage device for level k+1.   \n",
    "Data is always copied back and forth between the levels in block size **transfer units**.  \n",
    "**The block size is fixed between adjacent levels in the hierarchy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a program needs a specific data object from a lower leve, it first looks for data stored in the current level. If it is found, we have a **cache hit**. The program then reads the data directly from the current level, which is faster than reading it from the lower level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not cached at the current level, we have a **cache miss**. When there is a miss, the cache at the current level fetches the data from the lower level, maybe overwriting an existing block if the current level's cache is fll.   \n",
    "The process of overwriting a block is knowen as **replacing** the block. The block that is replaced is referred to as the **victim block**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of different kinds of caches.  \n",
    "The first is called a **cold miss**. This happens when misses occur on an empty cache, which is sometimes referred to as a **cold cache**. Cold misses are important since they are often transient events that might not occur in a **steady state** which is when the cache has been **warmed up** by repeated memory accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is a miss, the cache at the current level must implement a **placement policy** in order to place the block it has retrieved. The most flexible policy is to allow it to be stored in any block.   \n",
    "Restrictive placement policies lead to a **conflict miss** in which the cache is large enough to hold the data objects, but since they map to the same cache block, the cache keeps missing. \n",
    "Programs often run as a sequence of phases where each phase accesses some reasonably constant set of cache blocks, which is known as the **working set**. For example a nested loop might access the same elements over and over again.    \n",
    "Then the size of the working set exceeds the size of the cache, the cache will experience what is known as a **capacity miss**. In other words, the cache is just too small to handle the working set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Cache Memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the increasing gap between CPU and main memory, systems designers had to make a small SRAM **cache memory**, an L1 cache between the CPU register file and the main memory.   \n",
    "As the performance gap between the CPU and main memory continued to increase, systems designers responded by inserting a larger cache called an **L2 cache** between the L1 cache and main memory that could be accessed in **10 clock cycles**.   \n",
    "Many modern systems include an even larger cache called an **L3 cache** which sits between the L2 cache and main memory and can be accessed in about **50 cycles**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 Generic Cache Memory Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a computer system in which each memory address has m bits that form 2^m unique addresses.  \n",
    "A cache for such a machine is organized as array of 2^s cache sets.   \n",
    "Each set consists of E **cache lines**.  \n",
    "Each line consists of a **data block of 2^b bytes,** a **valid byte** that indicates whether or not the line consists of meaningful information, and m-b+s **tag bits** that identifies the block stored in the cache line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cache's organization can be characterized by the tuple (S, E, B, m). The size of a cache is stated as the size of all the blocks. the tag bits are not included. Thus, the equation to find the size is **S x E x B.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the CPU is instructed to read a word from address A of the main memory, it sends address A to the cache. If the cache is holding a copy of the word at address A, the data is sent back to the CPU.   \n",
    "The cache is organized so that it can find the word by inspecting the bits of the address in a fashion similar to a hash table with a simple hash function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each address contains a tag, a set index and a block offset.  \n",
    "In order to find the data:  \n",
    "First, we must identify the set. Each set is partitioned into **three fields, the valid byte, the tag, and then the data in the cache block**. The **s set index bits** in A identifies the array of s sets. It also tells us which set the word must be stored in.   \n",
    "Once we know what which set the word is in, the tag bits in A tells us which line in the set contains the word. A line in the set contains the word if the valid bit is set and the tag bits match the tag bits in A.   \n",
    "Once we have found the line, the **b block offset bits** give us the offset of the word inthe B-byte data block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 Direct-Mapped Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caches are grouped into different classes based on the number of cache lines per set. A cache with exactly one line per set is known as a **Direct Mapped Cache**. Direct mapped caches are the simplest both to implement and undetstand.   \n",
    "The process that a cache goes through of determining whether a request is a hit or miss consists of three steps: **set selection, line matching, and word extraction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cache extracts the s set index bits fro, tje middle of the address. These bits are interpreted as an unsigned integer that corresponds to a set number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected a set i, the next step is to determine if a copy of the word is stored in one of the cache lines. In a direct mapped cache, this is easy since there is only one line per set. A copy is contained in the line if and only if the valid bit is set and the tag in the cache line matches the tag in the address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a hit, we know that w is somewhere in the block. The last step is to detemine **where the desired word starts in the block**. The block offset bits provides us the offset of the first byte in the desired word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we had a direct mapped cache described by:  \n",
    "(S, E, B, m) = (4,1,2,4).   \n",
    "This cache has 4 sets, one line per set, 2 bytes per block, and 4 bit addresses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conflict misses are common in real programs and case baffling performance problems. Conflict misses in direct mapped caches usually occur when programs access arrays whos sizes are power of 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float dotprod(float x[8], float y[8]){\n",
    "    float sum = 0.0;\n",
    "    int i;\n",
    "    for (i = 0; i < 8; i++)\n",
    "        sum += x[i]*y[i]\n",
    "    return sum\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function has good spatial locality, so we might expect it to enjoy a number of cache hits. However, this is not always true.  \n",
    "Suppose floats are 4 bytes that is loaded into 32 bytes of contiguous memory starting at address 0, and that y starts immediately after x at address 32.  \n",
    "Suppose that block is 16 bytes, which is large enough to hold 4 floats, and that the cache consists of two sets for a total size of 32 bytes.     \n",
    "At run time, the first iteration of the loop references x[0], a miss that contains the block containg x[0]-x[3] to be loaded into set 0. This is a cold miss. The next reference is to y[0] which will also cause another cold miss causing y[0]-y[3] to be copied into set 0. Nowe we have a conflict miss, and each subsequent reference to x and y will result in a conflict miss as we thrash between between blocks of x and y.    \n",
    "The term **thrashing** describes any situation where the cache is repeatedly loading and evicting the same et of cache blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though a program has good spatial locality, we have room in the cache to hold the blocks for both x[i] and y[i], each reference results in a conflic miss since both blocks map the the same cache set. This thrash can result in a slowdown by a factor of 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One easy way to fix thrashing is to put some bytes of padding at the end of each array.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 Set Associative Caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Writing Cache-Friendly code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmers try to write code that is cache friendly, so the basic approach is to:   \n",
    "**Make the common case go fast**: Programs spend most of the time on a few core functions. Focus on the inner loops of the core functions.   \n",
    "**Minimize the number of cache misses in the loop**: Loops with better miss rates will run faster assuming all other things being equal (i.e. # of loop stores and loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Higher miss rates can have a significant impact on the running time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
